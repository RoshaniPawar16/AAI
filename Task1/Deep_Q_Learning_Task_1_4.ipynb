{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmsPn13yfKE1RjyMQIYFPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshaniPawar16/AAI/blob/main/Task1/Deep_Q_Learning_Task_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeR2n6zZNu4O",
        "outputId": "f58695fd-f1be-4298-be44-dd3331fa98dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827624 sha256=df845ff08c7c55c1cb438e635883d9c53329f0eda7d74ea11834d3ab6de0ee6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHDdWaLUNNxm",
        "outputId": "37d9a780-60b8-467f-9d04-49fb664e5b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1001/100000 [02:21<4:03:54,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000: Reward = 0.0, Epsilon = 0.9057890438555999, Steps = 16, Adjusted Reward = -81.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 2000/100000 [05:53<5:41:03,  4.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2000: Reward = 0.0, Epsilon = 0.820543445547202, Steps = 12, Adjusted Reward = -80.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 3000/100000 [09:57<6:30:48,  4.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 3000: Reward = 0.0, Epsilon = 0.7434100384749007, Steps = 6, Adjusted Reward = -92.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 4000/100000 [14:42<7:22:49,  3.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 4000: Reward = 0.0, Epsilon = 0.6736168455752829, Steps = 29, Adjusted Reward = -85.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 5000/100000 [19:51<8:53:15,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5000: Reward = 0.0, Epsilon = 0.6104653531155071, Steps = 35, Adjusted Reward = -67.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 6000/100000 [25:35<8:43:26,  2.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 6000: Reward = 0.0, Epsilon = 0.5533235197330861, Steps = 17, Adjusted Reward = -78.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 7000/100000 [31:59<10:23:45,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 7000: Reward = 0.0, Epsilon = 0.5016194507534953, Steps = 19, Adjusted Reward = -70.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 8000/100000 [39:03<10:30:18,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 8000: Reward = 0.0, Epsilon = 0.45483567447604933, Steps = 33, Adjusted Reward = -67.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 9000/100000 [46:47<12:08:36,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 9000: Reward = 0.0, Epsilon = 0.4125039631431931, Steps = 9, Adjusted Reward = -91.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 10000/100000 [55:11<15:05:24,  1.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10000: Reward = 0.0, Epsilon = 0.3742006467597279, Steps = 15, Adjusted Reward = -87.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 11000/100000 [1:04:00<12:51:22,  1.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 11000: Reward = 0.0, Epsilon = 0.3395423728610988, Steps = 53, Adjusted Reward = -35.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 12000/100000 [1:13:34<15:17:33,  1.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 12000: Reward = 0.0, Epsilon = 0.3081822697930801, Steps = 65, Adjusted Reward = -33.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 13000/100000 [1:24:07<15:47:50,  1.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 13000: Reward = 0.0, Epsilon = 0.27980647510367246, Steps = 105, Adjusted Reward = -15.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 14000/100000 [1:35:33<19:57:34,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 14000: Reward = 0.0, Epsilon = 0.2541309943021904, Steps = 76, Adjusted Reward = -19.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 15000/100000 [1:48:05<19:09:24,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 15000: Reward = 0.0, Epsilon = 0.23089885854694553, Steps = 22, Adjusted Reward = -93.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 16000/100000 [2:02:21<24:37:47,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 16000: Reward = 0.0, Epsilon = 0.20987755281470882, Steps = 9, Adjusted Reward = -90.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 17000/100000 [2:17:58<26:09:32,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 17000: Reward = 0.0, Epsilon = 0.19085668881220727, Steps = 88, Adjusted Reward = -25.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 18000/100000 [2:34:49<23:39:14,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 18000: Reward = 0.0, Epsilon = 0.17364589933937066, Steps = 95, Adjusted Reward = -49.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 19000/100000 [2:52:52<26:04:38,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 19000: Reward = 0.0, Epsilon = 0.1580729330304087, Steps = 38, Adjusted Reward = -56.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 20000/100000 [3:11:28<26:48:57,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20000: Reward = 0.0, Epsilon = 0.1439819304042466, Steps = 14, Adjusted Reward = -75.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 21000/100000 [3:30:04<23:59:44,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 21000: Reward = 0.0, Epsilon = 0.13123186397045208, Steps = 41, Adjusted Reward = -58.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 22000/100000 [3:48:52<22:28:23,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 22000: Reward = 0.0, Epsilon = 0.11969512677871053, Steps = 25, Adjusted Reward = -68.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 23000/100000 [4:07:50<23:55:32,  1.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 23000: Reward = 0.0, Epsilon = 0.10925625528557566, Steps = 134, Adjusted Reward = 41.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 24000/100000 [4:26:55<21:38:59,  1.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 24000: Reward = 0.0, Epsilon = 0.09981077375651838, Steps = 22, Adjusted Reward = -77.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 25000/100000 [4:46:08<22:55:46,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 25000: Reward = 0.0, Epsilon = 0.0912641486376598, Steps = 20, Adjusted Reward = -64.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 26000/100000 [5:05:37<23:16:16,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 26000: Reward = 0.0, Epsilon = 0.08353084243219053, Steps = 96, Adjusted Reward = 1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 27000/100000 [5:25:12<28:29:40,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 27000: Reward = 0.0, Epsilon = 0.07653345761235225, Steps = 151, Adjusted Reward = 42.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 28000/100000 [5:45:16<22:45:29,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 28000: Reward = 0.0, Epsilon = 0.07020196199896576, Steps = 18, Adjusted Reward = -73.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 29000/100000 [6:05:35<25:49:46,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 29000: Reward = 0.0, Epsilon = 0.06447298785584314, Steps = 66, Adjusted Reward = -22.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 30000/100000 [6:26:04<24:01:32,  1.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 30000: Reward = 0.0, Epsilon = 0.05928919768418531, Steps = 137, Adjusted Reward = 32.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 31000/100000 [6:45:45<20:58:18,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 31000: Reward = 0.0, Epsilon = 0.05459871036962222, Steps = 76, Adjusted Reward = -16.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 32%|███▏      | 32000/100000 [7:05:13<21:53:30,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 32000: Reward = 0.0, Epsilon = 0.05035458193858255, Steps = 96, Adjusted Reward = -9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 32969/100000 [7:24:05<19:56:17,  1.07s/it]"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "np.random.seed(4)\n",
        "\n",
        "\n",
        "# Create the environment with is_slippery=True\n",
        "random_map = generate_random_map(size=10, p=0.3)\n",
        "env = gym.make(\"FrozenLake-v1\", desc=random_map, render_mode=\"rgb_array\", is_slippery=True)\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "# Named tuple representing a single transition in our environment\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "nspace = env.observation_space.n\n",
        "naction = env.action_space.n\n",
        "print(nspace)\n",
        "\n",
        "# Cyclic buffer of bounded size that holds and samples the transitions observed recently\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.priorty = []\n",
        "        self.position = 0\n",
        "        self.alpha = 0.6\n",
        "\n",
        "    def push(self, *args, step):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        priority = (step + 1e-5) ** self.alpha\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "            self.priorty.append(priority)\n",
        "        else:\n",
        "            self.priorty[self.position] = priority\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "        # self.priorty[self.position] = priority\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # return random.sample(self.memory, batch_size)\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False, p=self.priorty / np.sum(self.priorty))\n",
        "        return [self.memory[i] for i in indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, action_dim)\n",
        "            # nn.Linear(state_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        # Initialize weights using Xavier/Glorot initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "def get_action(state, epsilon):\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\n",
        "    if exp_exp_tradeoff > epsilon:\n",
        "        with torch.no_grad():\n",
        "            action = policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        # action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "        action = env.action_space.sample()\n",
        "        action = torch.tensor(np.array(action), device=device).view(1, 1)\n",
        "    return action\n",
        "\n",
        "# Hyperparameters\n",
        "total_episodes = 50000*2\n",
        "max_steps = 500\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "target_update = 5\n",
        "train_frequency = 1\n",
        "train_epochs = 10\n",
        "# goal_position = env.observation_space.n - 1  # Goal is the last position in the grid\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.0001\n",
        "\n",
        "policy_net = DQN(nspace, naction).to(device)\n",
        "target_net = DQN(nspace, naction).to(device)\n",
        "\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Initialize the replay buffer\n",
        "memory = ReplayMemory(500000)\n",
        "\n",
        "def one_hot_encode(state, num_classes):\n",
        "    one_hot_array = np.zeros(num_classes, dtype=np.float32)\n",
        "    one_hot_array[state] = 1\n",
        "    return one_hot_array\n",
        "\n",
        "# Manhattan distance heuristic to guide the agent towards the goal\n",
        "def manhattan_distance(state):\n",
        "    goal_position = env.observation_space.n - 1  # Goal is the last position in the grid\n",
        "    grid_size = int(np.sqrt(nspace))\n",
        "    state_x, state_y = state % grid_size, state // grid_size\n",
        "    goal_x, goal_y = goal_position % grid_size, goal_position // grid_size\n",
        "    return abs(goal_x - state_x) + abs(goal_y - state_y)\n",
        "\n",
        "\n",
        "def trainDQN(policy_net, target_net, optimizer, epsilon, total_episodes, memory):\n",
        "    rewards = []\n",
        "    steps_moved = []\n",
        "    new_rewards = []\n",
        "    for episode in tqdm(range(1, total_episodes + 1)):\n",
        "        current_state, _ = env.reset()\n",
        "        state = one_hot_encode(current_state, nspace)\n",
        "        state = torch.tensor(np.array([state]), device=device)\n",
        "        total_rewards = 0\n",
        "        actual_rewards = 0\n",
        "        total_steps = 0\n",
        "        visited_states = set()\n",
        "        visited_states.add(current_state)\n",
        "        for step in range(max_steps):\n",
        "            action = get_action(state, epsilon)\n",
        "            next_state, org_reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "            # Reward shaping (adjusted for simplicity)\n",
        "            if done:\n",
        "                reward = 100 if org_reward == 1 else -100\n",
        "            else:\n",
        "                # Reward for getting closer to the goal\n",
        "                reward = 0\n",
        "                distance_to_goal = manhattan_distance(current_state)\n",
        "                next_distance_to_goal = manhattan_distance(next_state)\n",
        "\n",
        "                if next_distance_to_goal < distance_to_goal:\n",
        "                    reward = 2.0  # Double the reward for moving closer\n",
        "                elif next_distance_to_goal > distance_to_goal:\n",
        "                    reward = -0.1 * next_distance_to_goal  # Minor penalty for moving away from the goal\n",
        "\n",
        "                # Additional checks to avoid loops and repeated states\n",
        "                if next_state in visited_states or next_state < current_state:\n",
        "                    reward -= 1.0  # Penalty for revisiting states to avoid loops\n",
        "                else:\n",
        "                    reward += 0.5  # Small reward for exploring new states\n",
        "\n",
        "                visited_states.add(next_state)  # Track visited states to prevent loops\n",
        "\n",
        "            next_state = one_hot_encode(next_state, nspace)\n",
        "            total_rewards += org_reward\n",
        "            actual_rewards += reward\n",
        "            total_steps += 1\n",
        "\n",
        "\n",
        "            reward = torch.tensor(np.array([reward]), device=device)\n",
        "            done = torch.tensor(np.array([int(done)]), device=device)\n",
        "            next_state = torch.tensor(np.array([next_state]), device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                current_q = policy_net(state).gather(1, action)\n",
        "                next_q = target_net(next_state).max(1)[0] * (1 - done)\n",
        "                td_error = abs(current_q - (reward + gamma * next_q)).item()\n",
        "\n",
        "\n",
        "            memory.push(state, action, next_state, reward, done, step=td_error)\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if episode % train_frequency == 0 and len(memory) >= batch_size:\n",
        "            for _ in range(train_epochs):\n",
        "                transitions = memory.sample(batch_size)\n",
        "                batch = Transition(*zip(*transitions))\n",
        "\n",
        "                state_batch = torch.cat(batch.state)\n",
        "                action_batch = torch.cat(batch.action)\n",
        "                next_state_batch = torch.cat(batch.next_state)\n",
        "                reward_batch = torch.cat(batch.reward)\n",
        "                done_batch = torch.cat(batch.done)\n",
        "\n",
        "                # Current Q values for chosen actions\n",
        "                state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "                # Double DQN: action selection by policy_net, evaluation by target_net\n",
        "                with torch.no_grad():\n",
        "                    next_actions = policy_net(next_state_batch).max(1)[1].unsqueeze(1)\n",
        "                    next_state_values = target_net(next_state_batch).gather(1, next_actions).squeeze()\n",
        "\n",
        "                # Calculate the target Q values\n",
        "                target_q_values = (next_state_values * (1 - done_batch) * gamma) + reward_batch\n",
        "\n",
        "                # Loss calculation and backpropagation\n",
        "                loss = F.smooth_l1_loss(state_action_values, target_q_values.unsqueeze(1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "        rewards.append(total_rewards)\n",
        "        steps_moved.append(total_steps)\n",
        "        new_rewards.append(actual_rewards)\n",
        "        if episode % 1000 == 0:\n",
        "            print(f\"Episode {episode}: Reward = {total_rewards}, Epsilon = {epsilon}, Steps = {total_steps}, Adjusted Reward = {actual_rewards}\")\n",
        "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "            torch.save(policy_net.state_dict(), f'checkpoints/policy_net-{episode}.pth')\n",
        "            torch.save(target_net.state_dict(), f'checkpoints/target_net-{episode}.pth')\n",
        "\n",
        "    print(\"Score over time: \" + str(sum(rewards) / total_episodes))\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "    fig.tight_layout(pad=4.0)\n",
        "\n",
        "    # Plot Reward at Each Episode\n",
        "    axes[0, 0].plot(rewards)\n",
        "    axes[0, 0].set_xlabel('Episode')\n",
        "    axes[0, 0].set_ylabel('Reward')\n",
        "    axes[0, 0].set_title('Reward at Each Episode')\n",
        "\n",
        "    # Plot Steps Moved at Each Episode\n",
        "    axes[0, 1].plot(steps_moved)\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Steps Moved')\n",
        "    axes[0, 1].set_title('Steps Moved at Each Episode')\n",
        "\n",
        "    # Plot Adjusted Reward at Each Episode\n",
        "    axes[1, 0].plot(new_rewards)\n",
        "    axes[1, 0].set_xlabel('Episode')\n",
        "    axes[1, 0].set_ylabel('Adjusted Reward')\n",
        "    axes[1, 0].set_title('Adjusted Reward at Each Episode')\n",
        "\n",
        "    # Plot Reward at Each Episode (Moving Average)\n",
        "    moving_average = np.convolve(rewards, np.ones(1000) / 1000, mode='valid')\n",
        "    axes[1, 1].plot(moving_average)\n",
        "    axes[1, 1].set_xlabel('Episode')\n",
        "    axes[1, 1].set_ylabel('Reward')\n",
        "    axes[1, 1].set_title('Reward at Each Episode (Moving Average)')\n",
        "\n",
        "    # Plot Adjusted Reward at Each Episode (Moving Average)\n",
        "    moving_average = np.convolve(new_rewards, np.ones(1000) / 1000, mode='valid')\n",
        "    axes[2, 0].plot(moving_average)\n",
        "    axes[2, 0].set_xlabel('Episode')\n",
        "    axes[2, 0].set_ylabel('Reward')\n",
        "    axes[2, 0].set_title('Adjusted Reward at Each Episode (Moving Average)')\n",
        "\n",
        "    # Plot Steps Moved at Each Episode (Moving Average)\n",
        "    moving_average = np.convolve(steps_moved, np.ones(1000) / 1000, mode='valid')\n",
        "    axes[2, 1].plot(moving_average)\n",
        "    axes[2, 1].set_xlabel('Episode')\n",
        "    axes[2, 1].set_ylabel('Steps Moved')\n",
        "    axes[2, 1].set_title('Steps Moved at Each Episode (Moving Average)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "import base64, io, os\n",
        "\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "\n",
        "os.makedirs(\"video\", exist_ok=True)\n",
        "\n",
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = 'video/{}.mp4'.format(env_name)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def show_video_of_model(env_name):\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n",
        "    state, prob = env.reset()\n",
        "    state = one_hot_encode(state, nspace)\n",
        "    state = torch.tensor(np.array([state]), device=device)\n",
        "    done = False\n",
        "    epsilon = 0\n",
        "    tl = 0\n",
        "    max_steps = 500\n",
        "    policy_net.load_state_dict(torch.load(f'checkpoints/policy_net-{total_episodes}.pth'))\n",
        "    for t in range(max_steps+1):\n",
        "        vid.capture_frame()\n",
        "        with torch.no_grad():\n",
        "            action = policy_net(state).max(1)[1].view(1, 1)\n",
        "        next_state, reward, done, info, _ = env.step(action.item())\n",
        "        next_state = one_hot_encode(next_state, nspace)\n",
        "        tl+=reward\n",
        "        next_state = torch.tensor(np.array([next_state]), device=device)\n",
        "        # print(f\"state: {state}, action: {action.item()}\",next_state, reward, done)\n",
        "        state = next_state\n",
        "        # break in the next iteration\n",
        "\n",
        "    vid.close()\n",
        "    env.close()\n",
        "    print(\"Final reward:\", tl)\n",
        "    return tl\n",
        "\n",
        "rewards = trainDQN(policy_net, target_net, optimizer, epsilon, total_episodes, memory)\n",
        "\n",
        "show_video_of_model(\"FrozenLake-v1\")\n",
        "\n",
        "show_video(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_video_of_model(\"FrozenLake-v1\")\n",
        "\n",
        "show_video(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "id": "1RcmK9zeNa53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VF-eHFohDa10"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}