{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshaniPawar16/AAI/blob/main/Task1/Deep_Q_Learning_Task_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(policy_net, episodes=1000, max_steps=500):\n",
        "    \"\"\"Evaluate the performance of the trained agent.\"\"\"\n",
        "    success_count = 0\n",
        "    total_rewards = 0\n",
        "    total_adjusted_rewards = 0\n",
        "    total_steps = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        current_state, _ = env.reset()\n",
        "        state = one_hot_encode(current_state, nspace)\n",
        "        state = torch.tensor(np.array([state]), device=device)\n",
        "        episode_rewards = 0\n",
        "        episode_adjusted_rewards = 0\n",
        "        steps = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state).max(1)[1].view(1, 1)\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action.item())\n",
        "            episode_rewards += reward\n",
        "\n",
        "            # Adjusted reward calculation\n",
        "            distance_to_goal = manhattan_distance(current_state)\n",
        "            next_distance_to_goal = manhattan_distance(next_state)\n",
        "            adjusted_reward = reward\n",
        "            if done and reward == 1:\n",
        "                adjusted_reward += 100  # Goal reached\n",
        "            elif done and reward == 0:\n",
        "                adjusted_reward -= 100  # Fell into a hole\n",
        "            elif next_distance_to_goal < distance_to_goal:\n",
        "                adjusted_reward += 2\n",
        "            else:\n",
        "                adjusted_reward -= 0.1 * next_distance_to_goal\n",
        "            episode_adjusted_rewards += adjusted_reward\n",
        "\n",
        "            current_state = next_state\n",
        "            next_state = one_hot_encode(next_state, nspace)\n",
        "            state = torch.tensor(np.array([next_state]), device=device)\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                if reward == 1:\n",
        "                    success_count += 1\n",
        "                break\n",
        "\n",
        "        total_rewards += episode_rewards\n",
        "        total_adjusted_rewards += episode_adjusted_rewards\n",
        "        total_steps += steps\n",
        "\n",
        "    avg_success_rate = (success_count / episodes) * 100\n",
        "    avg_reward = total_rewards / episodes\n",
        "    avg_adjusted_reward = total_adjusted_rewards / episodes\n",
        "    avg_steps = total_steps / episodes\n",
        "\n",
        "    print(f\"Evaluation Results over {episodes} episodes:\")\n",
        "    print(f\"Success Rate: {avg_success_rate:.2f}%\")\n",
        "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
        "    print(f\"Average Adjusted Reward: {avg_adjusted_reward:.2f}\")\n",
        "    print(f\"Average Steps Taken: {avg_steps:.2f}\")\n",
        "\n",
        "    return avg_success_rate, avg_reward, avg_adjusted_reward, avg_steps\n",
        "\n",
        "\n",
        "# Evaluate the agent\n",
        "evaluate_agent(policy_net)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkjbw0SPCnX1",
        "outputId": "7ef1dfaa-747e-4e06-fa09-fc84947c8234"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results over 1000 episodes:\n",
            "Success Rate: 0.00%\n",
            "Average Reward: 0.00\n",
            "Average Adjusted Reward: -109.71\n",
            "Average Steps Taken: 132.41\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.0, -109.70519999999996, 132.414)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbqQ0BfGCn1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}